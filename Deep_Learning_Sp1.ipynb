{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8t731kSZsFD/9sBfh44UH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prathmesh234/ConvolutionalNeuralNetwork/blob/main/Deep_Learning_Sp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a Neural Network?\n",
        "House pricing prediction - Size x will be a feature and Price Y is what we get.\n",
        "Multiple features  - size, bedrooms etc\n",
        "ReLU - Rectified Linear Unit\n",
        "\n",
        "Each neuron can respond to a specific feature. Every input feature is connected to the other one  - densely populated NN.\n",
        "\n",
        "\n",
        "Supervised Learning  - Input , output based. Image classification - CNN. Audio - 1 dimension temporal sequence : RNN.\n",
        "\n",
        "Structured and Unstructured Data  - Well defined vs audio/image - not well defined.\n",
        "\n"
      ],
      "metadata": {
        "id": "l5NVp2vtBHXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale drives the deep learning progress.Scale is also depending on the labeled data. Scale - Data + Computation + algorithms.\n",
        "\n",
        "Eg - Going from Sigmoid -> ReLU : The plateau in the sigmoid does not help will resolving the gradient descent. However, ReLU did.\n",
        "\n"
      ],
      "metadata": {
        "id": "mUaZKfSfEFvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Week 2**\n",
        "\n",
        "Binary Classification -\n",
        "\n",
        "Logistic Regression - Let us say we have an image with 64x64  - and we can take a feature vector of the image it will be 64x64x3(RBG) = 12288 - dimension of the vector. m - training examples (x1,y1), (x2,y2) .......\n",
        "\n",
        "X = [x1,x2,......] - width : m, height - n\n",
        "X : Rnxm. Y = R1xm\n",
        "x.shape - finding the vector's shape\n",
        "\n",
        "**logistic Regression **- y = sigmoied(x^T + b). (cannot be negative and there is a chance it could be 0)\n",
        "\n",
        "sigmoid(x) = 1/1+e^-x. x is large - sigmoid - 1. x is small - sigmoid - 0.\n",
        "\n",
        "Cost function - Loss function or the error function. MSE does not work as gradient descent cannot find a local minima.\n",
        "\n",
        "We use -(ylogy_pred + (1-y)log(1-y_pred)). Loss_fn - single training example. Cost functn - sum of all the loss func throught out the entire training dataset.\n",
        "\n",
        "\n",
        "**Logistic regression as a Neural Network - ** -\n",
        "j(w) - functn you want to minimize. w:= (updating) w- alpha d J(w)/dw. Derivative. Akpha - learning rate.\n",
        "\n",
        "\n",
        "Computation Graph - Backprop uses derivative of a computation graph.\n",
        "\n",
        "dvar = dJ/dvar.\n",
        "\n",
        "Implementing gradient descent for logistic regression.\n",
        "\n",
        "Gradient Descent on m examples -\n",
        "\n",
        "-> J = 0, dw1 = 0, dw2 = 0, db =0\n",
        "\n",
        "for i to m:\n",
        " zi = wTxi + b\n",
        " ai = sig(zi)\n",
        " loss = -[L(a,y)]\n",
        " dzi = ai - yi\n",
        " dw1 += x1idzi....\n",
        "\n",
        " All you have to know - w1 = w1 - alphadw1, w2...., b...... This is backprop.\n",
        "\n",
        "\n",
        " Vectorization - Getting rid of the for loop as shown above."
      ],
      "metadata": {
        "id": "JoO2opmhHMIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##vectorization\n",
        "\n",
        "import numpy as np\n",
        "a = np.array([1,2,3,4])\n",
        "print(a)\n",
        "\n",
        "##Just remember vectorized version runs almost 300x faster."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APTD449vVajz",
        "outputId": "e0c73480-d7be-454f-b422-4948511ccf0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "a = np.random.rand(1000000)\n",
        "b = np.random.rand(1000000)\n",
        "tic = time.time()\n",
        "c = np.dot(a,b)\n",
        "toc = time.time()\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUjkD8Z7VheL",
        "outputId": "964ee4d6-e174-4715-a0c5-c78ac81c0326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249980.55636429758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "A = np.array([[56.0,0.0,4.4,68.0],\n",
        "              [1.2,104.0,52.0,8.0],\n",
        "              [1.0,135.0,99.0,0.9]])\n",
        "print(A)"
      ],
      "metadata": {
        "id": "bmy_Ve9FVpXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ff132f-9812-4ed3-b3b2-f1fc84b9f77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 56.    0.    4.4  68. ]\n",
            " [  1.2 104.   52.    8. ]\n",
            " [  1.  135.   99.    0.9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cal = A.sum(axis=0)\n",
        "print(cal)\n",
        "cal.shape\n",
        "##as you can see cal becomes a single row matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM_3T3RVmqQD",
        "outputId": "8be369cb-b752-4395-903f-021baab22501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 58.2 239.  155.4  76.9]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentage = 100*A/cal.reshape(1,4)\n",
        "print(percentage)\n",
        "##Above is broadcasting in matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E441C8Ebmtwe",
        "outputId": "573ec0ad-37d7-4919-b97c-c7d707cd7e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[96.21993127  0.          2.83140283 88.42652796]\n",
            " [ 2.06185567 43.51464435 33.46203346 10.40312094]\n",
            " [ 1.71821306 56.48535565 63.70656371  1.17035111]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.randn(2,3)\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKw_BvCcmxLW",
        "outputId": "b4c12e4f-6b74-4660-954a-053524242e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.17367706, -0.07673314,  0.50207158],\n",
              "       [ 0.63504976,  0.62961927, -1.40765031]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B = np.array([100,100,100])\n",
        "Z = A+B\n",
        "Z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuhUHsningib",
        "outputId": "eb0426c2-2b6d-4ac9-e35d-5465b1fd1e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 99.82632294,  99.92326686, 100.50207158],\n",
              "       [100.63504976, 100.62961927,  98.59234969]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.rand(5,1)\n",
        "print(A)\n",
        "A = np.dot(A,A.T)\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYwIuoQ7np4Q",
        "outputId": "f38a4704-8a89-49ec-eca8-bbbce9894a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.75061773]\n",
            " [0.97906083]\n",
            " [0.87029315]\n",
            " [0.61650434]\n",
            " [0.56736188]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.56342698, 0.73490042, 0.65325747, 0.46275909, 0.42587189],\n",
              "       [0.73490042, 0.9585601 , 0.85206994, 0.60359525, 0.55548179],\n",
              "       [0.65325747, 0.85206994, 0.75741017, 0.53653951, 0.49377116],\n",
              "       [0.46275909, 0.60359525, 0.53653951, 0.38007761, 0.34978106],\n",
              "       [0.42587189, 0.55548179, 0.49377116, 0.34978106, 0.3218995 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[2,1],[1,3]])\n",
        "a*a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiNcu5ZgpMH3",
        "outputId": "84a966c5-1b40-4a0a-a493-dede6b78102d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4, 1],\n",
              "       [1, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.random.randn(3,3)\n",
        "b = np.random.randn(3,1)\n",
        "a*b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_HBzz98qxyU",
        "outputId": "a46a5a42-a168-44b9-f38c-bfa4d0f59979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.58147095, -0.26229982,  0.5287682 ],\n",
              "       [-0.23181515, -0.03534474, -0.55557305],\n",
              "       [-0.04832295,  0.71795989, -0.32910311]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a= np.array([[1,1], [1,-1]])\n",
        "b= np.array([[2], [3]])\n",
        "c = a+b\n",
        "c\n"
      ],
      "metadata": {
        "id": "FfFWymODq_Q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654fefe8-5b09-4b86-bbfa-e24a879f46b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 3],\n",
              "       [4, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Week 3**\n",
        "\n",
        "Neural Network with one hidden layer. Input layer, hidden layer, output layer.\n",
        "\n",
        "Like logistic regression, but it repeats multiple times.\n",
        "\n",
        "For a neural network with one hidden layer,\n",
        "we will have the following equation for the generic representation of the z'th term which will be sent into the activation sigmoid function.\n",
        "\n",
        "zn[m] = wn[m]^T + bn[m] = sigmoid(zn[n])\n",
        "\n",
        "As you can see here n = node number and m is equal to the hidden layer number.\n",
        "\n",
        "\n",
        "**ACTIVATION FUNCTIONS**\n",
        "\n",
        "sigmoid(z[i]) - sigmoid function\n",
        "sigmoid func goes between 0 and 1.\n",
        "\n",
        "g(z[i]) - tanh function. Values between -1 and 1. Good for hidden layers.\n",
        "\n",
        "For binary classification we should use sigmoid as it should be between 0 and 1 only.\n",
        "\n",
        "ReLU function\n",
        "\n",
        "ReLU func is better than sigmoid for a simple reason. If the value of z[i] is too small then the slope will be very small too and acheiving gradient descent will be harder. ReLU only gives value of 0 or 1 so it is generally better as it activates more on non 0 values.\n",
        "\n",
        "BUT WHY DO WE NEED A NON LINEAR ACTIVATION FUNCTION?\n",
        "\n",
        "The NN will only output a linear activation and not get the non linear values it.\n",
        "\n",
        "**GRADIENT DESCENT**\n",
        "\n",
        "Sigmoid function = g(z) = 1/1+e^-z\n",
        "\n",
        "z == 0\n",
        "g(z) = 0.5\n",
        "\n",
        "z == -1\n",
        "g(z) = 1/4.... =\n",
        "\n",
        "tanh funciton  = g(z) = e^z - e^-z/e^z + e^-z\n",
        "\n",
        "z == large, g'(z) = 0\n",
        "\n",
        "ReLU\n",
        "\n",
        "g(z) = max(0,z)\n",
        "\n",
        "Leaky ReLU\n",
        "\n",
        "g(z) = max(0.01, z)\n",
        "\n",
        "GRADIENT DESCENT\n",
        "\n",
        "Compute preds after cost func\n",
        "Compute the derivatives of the cost function.\n",
        "Repeat this until the parameters converge to a local minima.\n",
        "\n",
        "Backprop\n",
        "dz[2] = A[2] - Y\n",
        "dw[2] = 1/mdz[2]A[1]T\n",
        "db[2] = sum of all the dz[2]\n",
        "\n",
        "dz[1] = w[2]T dz[2] * g[2](z[1])\n",
        "\n",
        "Initializing weights randomly and why it is important\n",
        "\n",
        "We did it as 0 but we need to initialize it as random as possible. The hidden units will be computing the same exact function and they will be computing exactly the same function.\n",
        "\n",
        "So if we dont do it random, two hidden layers will have the same performance as one.\n",
        "\n",
        "Also we usually multiply the weights by a small number to keep it's value small so when we compute tanh or any other activation function, for a larger value it should activate.\n",
        "\n"
      ],
      "metadata": {
        "id": "6HD4aB9M30hZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WEEK 4**\n",
        "\n",
        "Deep L Neural Network\n",
        "\n",
        "Only a single layer is basically logistic regression. Very shallow model.\n",
        "\n",
        "Deep neural network - a lot more hidden layers than logistic regression.\n",
        "\n",
        "L = Number of layers in the NN,\n",
        "N^[L] = Number of units in layer L.\n",
        "N[2] = 3 (2nd layer has 3 neurons)\n",
        "N[0] = nx\n",
        "a^[L] = activations in layer l = g^[L](z^[L])\n",
        "\n",
        "\n",
        "Forward propogation in a deep network\n",
        "\n",
        "x = z^[1] = W^[1]x + b[1]\n",
        "a[1] = g[1](z[1]) - activations for layer 1\n",
        "\n",
        "z[2] = w[2]a[1] + b[2]\n",
        "a[2] = g[2](z[2]) ....\n",
        "\n",
        "z[l] = w[l]a[l-1] + b[l]\n",
        "a[l] = g[l](z[l])\n",
        "\n",
        "Stacking all the vectors up.\n",
        "\n",
        "Working through the dimensions\n",
        "\n",
        "dimensions = (number of vectors/neurons in layer l, 1)\n",
        "(3,1) = x(2,1) + b\n",
        "x = (3,2), b = (3,1)\n",
        "\n",
        "Basically the simplest equation is (this is for the dimensions of the weights)  - w[l] = (n[l],n[l-1])\n",
        "b[l] = (n[l],1)\n",
        "\n",
        "Dimension for the activation functions (like the product after the activation function)-\n",
        "\n",
        "z[l],a[l] = (n[l],1).\n",
        "\n",
        "Also, if l =0, (n[l],m), m is equal to the number of training examples\n",
        "\n",
        "Why deep representation of neural network -\n",
        "\n",
        "For image representation/ face recognition\n",
        "1) First layer could check the orientation of the edges\n",
        "2) Second could check the horizontal edges etc.\n",
        "3) Could group the edges together to form different detections like eyes etc.\n",
        "4) detect different faces.\n",
        "\n",
        "Audio - one layer could check for low level features, second could check the basic units of sound (phonemes)\n",
        "\n",
        "Simple -> ..... -> complext....\n",
        "\n",
        "Circuit theory and deep learning  - less l deep layers .., then we need more hidden units for the layers for equal performance. Specifically, it has to be exponentially large.\n",
        "\n",
        " **Building blocks of neural networks**\n",
        "\n",
        " Check the image which in the saved section in the folder.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uGUUDP7ayvji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dz[l] = w[l+1]^T dz[l+1]*g[l]'(z^[l])\n",
        "\n",
        "Summary\n",
        "\n",
        "x -> relu -> relu -> sigmoid -> y^ -> L(y^,y) and then we do backprop\n",
        "dw[1] db[1] <- dw[2] db[2] <- dw[3] db[3] = -y/a +(1-y)/1-a\n",
        "\n",
        "**Hyperparameters vs  Parameters**\n",
        "\n",
        "Parameters: W[1], b[1], W[2], b[2], W[3]...\n",
        "\n",
        "Hyperparams are  - learning rate (alpha), iterations , hidden layer, choice of activation functions\n",
        "\n",
        "Control the final value of the parameters.\n",
        "\n",
        "Applied deep learning  - empirical process. Idea -> code -> experiment -> idea....\n",
        "\n",
        "We will have to keep tweaking the hyperparams in order to see if the model is getting better.\n",
        "\n",
        "Deep learning vs Human brain\n",
        "\n",
        "Like the brain is an oversimplification.\n",
        "\n",
        "# **Shape of the Neural Network**\n",
        "\n",
        "Shape of W[l] is (n[l],n[l-1]) and shape of b[l] is (n[l],1)\n",
        "\n"
      ],
      "metadata": {
        "id": "A-nVW-A0T8ow"
      }
    }
  ]
}